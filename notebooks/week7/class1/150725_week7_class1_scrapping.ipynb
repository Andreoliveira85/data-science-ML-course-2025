{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393a365a",
   "metadata": {},
   "source": [
    "# Web Scraping for Beginners: Building Your First Data Pipeline\n",
    "\n",
    "## What You'll Learn Today\n",
    "By the end of this lesson, you'll be able to:\n",
    "- Extract information from websites automatically\n",
    "- Store that data in a database\n",
    "- Analyze the data to find interesting patterns\n",
    "\n",
    "## What is Web Scraping?\n",
    "Web scraping is like copying information from a digital book, but instead of doing it by hand, \n",
    "we write programs to do it automatically. It's useful for:\n",
    "- Collecting product prices from shopping websites\n",
    "- Gathering news headlines from multiple sources\n",
    "- Building datasets for research projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c20c8b",
   "metadata": {},
   "source": [
    "## Understanding Web Pages: HTML Basics\n",
    "\n",
    "Before we can scrape websites, we need to understand how web pages are structured.\n",
    "\n",
    "### HTML is Like a Filing System\n",
    "Think of HTML as a filing system where information is organized in containers (called \"tags\").\n",
    "\n",
    "### Essential HTML Tags for Scraping\n",
    "- `<h1>, <h2>, <h3>` - Headlines (like chapter titles)\n",
    "- `<p>` - Paragraphs (like regular text)\n",
    "- `<div>` - Divisions (like folders that group things)\n",
    "- `<a>` - Links (like bookmarks to other pages)\n",
    "- `<table>` - Tables (like spreadsheets)\n",
    "\n",
    "### See HTML in Action\n",
    "Right-click on any webpage and select \"View Page Source\" to see the HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507732d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49922788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is HTML code (the structure of a webpage)\n",
    "simple_html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Welcome to My Blog</h1>\n",
    "        <p>This is my first blog post.</p>\n",
    "        <p>This is my second blog post.</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b594d257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is my HTML code:\n",
      "\n",
      "<html>\n",
      "    <body>\n",
      "        <h1>Welcome to My Blog</h1>\n",
      "        <p>This is my first blog post.</p>\n",
      "        <p>This is my second blog post.</p>\n",
      "    </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is my HTML code:\")\n",
    "print(simple_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34782882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07cb0b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<html>\n",
       "<body>\n",
       "<h1>Welcome to My Blog</h1>\n",
       "<p>This is my first blog post.</p>\n",
       "<p>This is my second blog post.</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(simple_html, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec891124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title is : Welcome to My Blog\n"
     ]
    }
   ],
   "source": [
    "title = soup.find('h1')\n",
    "print(\"The title is :\", title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976cd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 2 paragraphs\n"
     ]
    }
   ],
   "source": [
    "## paragraphs\n",
    "paragraphs = soup.find_all('p')\n",
    "print(\"We found\", len(paragraphs), 'paragraphs')\n",
    "\n",
    "## method find finds the first division of the html; cf with find_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96acb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Today's News</h1>\n",
    "        <div class=\"article\">\n",
    "            <h2>Python Programming Growing in Popularity</h2>\n",
    "            <p>Python continues to be one of the most popular programming languages.</p>\n",
    "        </div>\n",
    "        <div class=\"article\">\n",
    "            <h2>Web Scraping Helps Researchers</h2>\n",
    "            <p>Scientists use web scraping to collect data for their studies.</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5cddf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main headline: Today's News\n",
      "- Python Programming Growing in Popularity\n",
      "- Web Scraping Helps Researchers\n",
      "- Python continues to be one of the most popular programming languages.\n",
      "- Scientists use web scraping to collect data for their studies.\n"
     ]
    }
   ],
   "source": [
    "## getting the headline\n",
    "soup = BeautifulSoup(news_html, 'html.parser')\n",
    "## find all articles and print them\n",
    "headline = soup.find(\"h1\")\n",
    "print(\"Main headline:\", headline.text)\n",
    "## find all paragraphs and print them\n",
    "article_headlines = soup.find_all(\"h2\")\n",
    "for headline in article_headlines:\n",
    "    print(\"-\", headline.text)\n",
    "\n",
    "paragraphs = soup.find_all(\"p\")\n",
    "for paragraph in paragraphs:\n",
    "    print(\"-\", paragraph.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23eafb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully we got the webpage\n",
      "No <title> tag found in the page\n",
      "Paragraphs \n",
      "          Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Perth, the begrimed, blistered old blacksmith, had not removed his portable forge to the hold again, after concluding his contributory work for Ahab's leg, but still retained it on deck, fast lashed to ringbolts by the foremast; being now almost incessantly invoked by the headsmen, and harpooneers, and bowsmen to do some little job for them; altering, or repairing, or new shaping their various weapons and boat furniture. Often he would be surrounded by an eager circle, all waiting to be served; holding boat-spades, pike-heads, harpoons, and lances, and jealously watching his every sooty movement, as he toiled. Nevertheless, this old man's was a patient hammer wielded by a patient arm. No murmur, no impatience, no petulance did come from him. Silent, slow, and solemn; bowing over still further his chronically broken back, he toiled away, as if toil were life itself, and the heavy beating of his hammer the heavy beating of his heart. And so it was.â€”Most miserable! A peculiar walk in this old man, a certain slight but painful appearing yawing in his gait, had at an early period of the voyage excited the curiosity of the mariners. And to the importunity of their persisted questionings he had finally given in; and so it came to pass that every one now knew the shameful story of his wretched fate. Belated, and not innocently, one bitter winter's midnight, on the road running between two country towns, the blacksmith half-stupidly felt the deadly numbness stealing over him, and sought refuge in a leaning, dilapidated barn. The issue was, the loss of the extremities of both feet. Out of this revelation, part by part, at last came out the four acts of the gladness, and the one long, and as yet uncatastrophied fifth act of the grief of his life's drama. He was an old man, who, at the age of nearly sixty, had postponedly encountered that thing in sorrow's technicals called ruin. He had been an artisan of famed excellence, and with plenty to do; owned a house and garden; embraced a youthful, daughter-like, loving wife, and three blithe, ruddy children; every Sunday went to a cheerful-looking church, planted in a grove. But one night, under cover of darkness, and further concealed in a most cunning disguisement, a desperate burglar slid into his happy home, and robbed them all of everything. And darker yet to tell, the blacksmith himself did ignorantly conduct this burglar into his family's heart. It was the Bottle Conjuror! Upon the opening of that fatal cork, forth flew the fiend, and shrivelled up his home. Now, for prudent, most wise, and economic reasons, the blacksmith's shop was in the basement of his dwelling, but with a separate entrance to it; so that always had the young and loving healthy wife listened with no unhappy nervousness, but with vigorous pleasure, to the stout ringing of her young-armed old husband's hammer; whose reverberations, muffled by passing through the floors and walls, came up to her, not unsweetly, in her nursery; and so, to stout Labor's iron lullaby, the blacksmith's infants were rocked to slumber. Oh, woe on woe! Oh, Death, why canst thou not sometimes be timely? Hadst thou taken this old blacksmith to thyself ere his full ruin came upon him, then had the young widow had a delicious grief, and her orphans a truly venerable, legendary sire to dream of in their after years; and all of them a care-killing competency.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "## scrap data through soap API\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://httpbin.org/html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully we got the webpage\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.find(\"title\")\n",
    "    if title:\n",
    "        print(\"Page title:\", title.text)\n",
    "    else:\n",
    "        print(\"No <title> tag found in the page\")\n",
    "\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    if paragraphs:\n",
    "        for p in paragraphs:\n",
    "             print(\"Paragraphs\", p.text)\n",
    "\n",
    "    else:\n",
    "        print(\"No paragraphs found\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to get the webpage\")\n",
    "    print(\"status code\", response.status_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326c792",
   "metadata": {},
   "source": [
    "## Scrapy: A More Powerful Scraping Framework\n",
    "\n",
    "### What is Scrapy?\n",
    "While Beautiful Soup is great for beginners, **Scrapy** is a more powerful framework for larger scraping projects. Think of it this way:\n",
    "- **Beautiful Soup**: Like a manual can opener - simple, direct, perfect for small tasks\n",
    "- **Scrapy**: Like an industrial food processor - more complex setup, but handles big jobs efficiently\n",
    "\n",
    "### When to Use Scrapy vs Beautiful Soup\n",
    "**Use Beautiful Soup when:**\n",
    "- Learning web scraping basics\n",
    "- Scraping a few pages occasionally\n",
    "- Simple, one-time data extraction\n",
    "- Working with data you already have\n",
    "\n",
    "**Use Scrapy when:**\n",
    "- Scraping hundreds or thousands of pages\n",
    "- Need to follow links automatically\n",
    "- Want built-in data export (CSV, JSON, databases)\n",
    "- Building production scraping systems\n",
    "- Need advanced features like handling cookies, sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Scrapy example (for comparison)\n",
    "# Note: Scrapy usually requires more setup, but here's a basic concept\n",
    "\n",
    "# First, you would install Scrapy:\n",
    "# pip install scrapy\n",
    "\n",
    "# Here's how a simple Scrapy spider looks:\n",
    "\"\"\"\n",
    "import scrapy\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = 'simple'\n",
    "    start_urls = ['https://httpbin.org/html']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Extract title\n",
    "        title = response.css('title::text').get()\n",
    "        \n",
    "        # Extract all paragraphs\n",
    "        paragraphs = response.css('p::text').getall()\n",
    "        \n",
    "        # Return structured data\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'paragraphs': paragraphs,\n",
    "            'url': response.url\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "# Scrapy differences from Beautiful Soup:\n",
    "# 1. Built-in HTTP handling (no need for requests library)\n",
    "# 2. CSS selectors and XPath support\n",
    "# 3. Automatic data export to files\n",
    "# 4. Built-in support for following links\n",
    "# 5. Concurrent processing (faster for many pages)\n",
    "\n",
    "print(\"Scrapy Example Code (above)\")\n",
    "print(\"Scrapy is more complex but more powerful for large projects\")\n",
    "print(\"For learning, Beautiful Soup is perfect!\")\n",
    "print(\"For production scraping, consider Scrapy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8198f76",
   "metadata": {},
   "source": [
    "## Quick Comparison: Beautiful Soup vs Scrapy\n",
    "\n",
    "| Feature | Beautiful Soup | Scrapy |\n",
    "|---------|---------------|---------|\n",
    "| **Learning Curve** | Easy | Moderate to Hard |\n",
    "| **Setup** | `pip install beautifulsoup4` | More configuration needed |\n",
    "| **Best For** | Learning, small projects | Large-scale scraping |\n",
    "| **Speed** | Good for small tasks | Faster for many pages |\n",
    "| **Data Export** | Manual (you write the code) | Built-in (CSV, JSON, etc.) |\n",
    "| **Error Handling** | Manual | Built-in retry logic |\n",
    "| **Following Links** | Manual | Automatic |\n",
    "| **Code Style** | Procedural (step by step) | Object-oriented (classes) |\n",
    "\n",
    "### Our Recommendation for Beginners\n",
    "**Start with Beautiful Soup** because:\n",
    "- Easier to understand how web scraping works\n",
    "- Simpler syntax and concepts\n",
    "- Better for learning the fundamentals\n",
    "- You can always upgrade to Scrapy later\n",
    "\n",
    "**Move to Scrapy when** you need to:\n",
    "- Scrape many websites regularly\n",
    "- Handle complex navigation between pages\n",
    "- Build production systems\n",
    "- Need better performance and error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46fe36",
   "metadata": {},
   "source": [
    "## Storing Scraped Data: Database Basics\n",
    "\n",
    "Now that we can extract data from websites, we need to store it somewhere useful.\n",
    "\n",
    "### Why Use a Database?\n",
    "- **Organization**: Keep data structured and searchable\n",
    "- **Persistence**: Data survives even if your program stops\n",
    "- **Efficiency**: Faster than reading/writing files repeatedly\n",
    "- **Scalability**: Can handle large amounts of data\n",
    "\n",
    "### SQLite: Perfect for Beginners\n",
    "- **No setup required**: Built into Python\n",
    "- **Single file**: Your entire database is one file\n",
    "- **SQL language**: Industry standard for working with data\n",
    "- **Portable**: Easy to share and backup\n",
    "\n",
    "### Basic Database Concepts\n",
    "- **Table**: Like a spreadsheet with rows and columns\n",
    "- **Column**: A category of data (like \"name\" or \"price\")\n",
    "- **Row**: A single record (like one person's information)\n",
    "- **Primary Key**: A unique identifier for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51f4c7",
   "metadata": {},
   "source": [
    "## Comprehensive Example: Book Store Scraping\n",
    "\n",
    "Now let's tackle a real-world scenario! We'll scrape a book catalog website that has:\n",
    "- **Multiple books** on each page (not just one item)\n",
    "- **Multiple pages** to navigate through\n",
    "- **Structured data** (titles, prices, ratings, availability)\n",
    "- **Perfect for analysis** (price trends, rating distributions, etc.)\n",
    "\n",
    "### Our Target: Books.toscrape.com\n",
    "This website is specifically designed for scraping practice - it's legal, ethical, and perfect for learning!\n",
    "\n",
    "**What we'll do:**\n",
    "1. Scrape multiple books from several pages\n",
    "2. Store all data in a proper SQL database\n",
    "3. Perform meaningful analysis on our dataset\n",
    "4. Create visualizations of our findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "818fe26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Books database created successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# First, let's create a proper database for our books\n",
    "def setup_books_database():\n",
    "    \"\"\"\n",
    "    Create a database with a proper schema for book data\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect('books_catalog.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create books table with all the fields we want to track\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS books (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            price_pounds REAL,\n",
    "            rating INTEGER,\n",
    "            availability TEXT,\n",
    "            in_stock INTEGER,\n",
    "            image_url TEXT,\n",
    "            page_number INTEGER,\n",
    "            scraped_at TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"âœ… Books database created successfully!\")\n",
    "\n",
    "# Set up our database\n",
    "setup_books_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f017e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://books.toscrape.com/catalogue/page-1.html\n",
      "âœ… Successfully scraped 20 books from page 1\n",
      "Found 20 books on page 1\n",
      "\n",
      "Example book:\n",
      "Title: A Light in the Attic\n",
      "Price: Â£51.77\n",
      "Rating: 3/5 stars\n",
      "In stock: 0\n"
     ]
    }
   ],
   "source": [
    "def convert_rating_to_number(rating_class):\n",
    "    \"\"\"\n",
    "    Convert rating class (like 'Three') to number (like 3)\n",
    "    \"\"\"\n",
    "    rating_map = {\n",
    "        'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5\n",
    "    }\n",
    "    \n",
    "    # Extract rating from class list\n",
    "    for class_name in rating_class:\n",
    "        if class_name in rating_map:\n",
    "            return rating_map[class_name]\n",
    "    return 0\n",
    "\n",
    "def extract_price(price_text):\n",
    "    \"\"\"\n",
    "    Extract numerical price from text like 'Â£51.77'\n",
    "    \"\"\"\n",
    "    # Remove currency symbol and convert to float\n",
    "    price_match = re.search(r'[\\d.]+', price_text)\n",
    "    return float(price_match.group()) if price_match else 0.0\n",
    "\n",
    "def extract_stock_info(availability_text):\n",
    "    \"\"\"\n",
    "    Extract stock number from text like 'In stock (22 available)'\n",
    "    \"\"\"\n",
    "    stock_match = re.search(r'\\((\\d+) available\\)', availability_text)\n",
    "    return int(stock_match.group(1)) if stock_match else 0\n",
    "\n",
    "def scrape_books_from_page(page_url, page_number):\n",
    "    \"\"\"\n",
    "    Scrape all books from a single page\n",
    "    \"\"\"\n",
    "    print(f\"Scraping page {page_number}: {page_url}\")\n",
    "    \n",
    "    try:\n",
    "        time.sleep(1)  # Be respectful to the server\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to get page {page_number}\")\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all book containers\n",
    "        book_containers = soup.find_all('article', class_='product_pod')\n",
    "        books_data = []\n",
    "        \n",
    "        for book in book_containers:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = book.find('h3').find('a')\n",
    "                title = title_element.get('title', 'No title')\n",
    "                \n",
    "                # Extract price\n",
    "                price_element = book.find('p', class_='price_color')\n",
    "                price = extract_price(price_element.text) if price_element else 0.0\n",
    "                \n",
    "                # Extract rating\n",
    "                rating_element = book.find('p', class_='star-rating')\n",
    "                rating = convert_rating_to_number(rating_element.get('class', [])) if rating_element else 0\n",
    "                \n",
    "                # Extract availability\n",
    "                availability_element = book.find('p', class_='instock availability')\n",
    "                availability = availability_element.text.strip() if availability_element else 'Unknown'\n",
    "                in_stock = extract_stock_info(availability)\n",
    "                \n",
    "                # Extract image URL\n",
    "                image_element = book.find('div', class_='image_container').find('img')\n",
    "                image_url = image_element.get('src', '') if image_element else ''\n",
    "                \n",
    "                book_data = {\n",
    "                    'title': title,\n",
    "                    'price_pounds': price,\n",
    "                    'rating': rating,\n",
    "                    'availability': availability,\n",
    "                    'in_stock': in_stock,\n",
    "                    'image_url': image_url,\n",
    "                    'page_number': page_number,\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "                books_data.append(book_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping individual book: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… Successfully scraped {len(books_data)} books from page {page_number}\")\n",
    "        return books_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page {page_number}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test scraping one page\n",
    "books_page_1 = scrape_books_from_page('https://books.toscrape.com/catalogue/page-1.html', 1)\n",
    "print(f\"Found {len(books_page_1)} books on page 1\")\n",
    "\n",
    "# Show first book as example\n",
    "if books_page_1:\n",
    "    first_book = books_page_1[0]\n",
    "    print(f\"\\nExample book:\")\n",
    "    print(f\"Title: {first_book['title']}\")\n",
    "    print(f\"Price: Â£{first_book['price_pounds']}\")\n",
    "    print(f\"Rating: {first_book['rating']}/5 stars\")\n",
    "    print(f\"In stock: {first_book['in_stock']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1641bde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting to scrape pages 1 to 3\n",
      "--------------------------------------------------\n",
      "Scraping page 1: https://books.toscrape.com/catalogue/page-1.html\n",
      "âœ… Successfully scraped 20 books from page 1\n",
      "ðŸ“š Stored 20 books from page 1\n",
      "Scraping page 2: https://books.toscrape.com/catalogue/page-2.html\n",
      "âœ… Successfully scraped 20 books from page 2\n",
      "ðŸ“š Stored 20 books from page 2\n",
      "Scraping page 3: https://books.toscrape.com/catalogue/page-3.html\n",
      "âœ… Successfully scraped 20 books from page 3\n",
      "ðŸ“š Stored 20 books from page 3\n",
      "--------------------------------------------------\n",
      "ðŸŽ‰ Scraping complete!\n",
      "ðŸ“Š Total books scraped and stored: 60\n"
     ]
    }
   ],
   "source": [
    "def store_books_in_database(books_list):\n",
    "    \"\"\"\n",
    "    Store a list of books in our database\n",
    "    \"\"\"\n",
    "    if not books_list:\n",
    "        return 0\n",
    "        \n",
    "    conn = sqlite3.connect('books_catalog.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert all books\n",
    "    for book in books_list:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO books (title, price_pounds, rating, availability, \n",
    "                             in_stock, image_url, page_number, scraped_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            book['title'], book['price_pounds'], book['rating'],\n",
    "            book['availability'], book['in_stock'], book['image_url'],\n",
    "            book['page_number'], book['scraped_at']\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return len(books_list)\n",
    "\n",
    "def scrape_multiple_pages(start_page=1, end_page=3):\n",
    "    \"\"\"\n",
    "    Scrape multiple pages and store all books\n",
    "    \"\"\"\n",
    "    all_books = []\n",
    "    total_stored = 0\n",
    "    \n",
    "    print(f\"ðŸš€ Starting to scrape pages {start_page} to {end_page}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page_url = f'https://books.toscrape.com/catalogue/page-{page_num}.html'\n",
    "        \n",
    "        # Scrape books from this page\n",
    "        books_from_page = scrape_books_from_page(page_url, page_num)\n",
    "        \n",
    "        if books_from_page:\n",
    "            # Store books in database\n",
    "            stored_count = store_books_in_database(books_from_page)\n",
    "            total_stored += stored_count\n",
    "            all_books.extend(books_from_page)\n",
    "            print(f\"ðŸ“š Stored {stored_count} books from page {page_num}\")\n",
    "        \n",
    "        # Be respectful - add delay between pages\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ðŸŽ‰ Scraping complete!\")\n",
    "    print(f\"ðŸ“Š Total books scraped and stored: {total_stored}\")\n",
    "    return all_books\n",
    "\n",
    "# Scrape 3 pages of books (about 60 books total)\n",
    "all_scraped_books = scrape_multiple_pages(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607ade3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
